<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name = "author" content = "Naledi Sibiya">
    <title>Third Blog</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name = "description" content="The URL blog">
    <link href="css/style.css" rel= "stylesheet" type= "text/css">
    <script src="JavaScript/Index.js" type="module" defer></script>
</head>


<body class = "blogposts">
<hearder>
        <h1>WEB ROBOTS</h1>
    </hearder>
    
    <figure>
        <img src= https://www.webfx.com/blog/wp-content/uploads/2019/07/what-is-web-crawler-gif.gif
         alt= "gif illustration a web robot tracking restuarants nearby"
         width="1500px"
         height= "500px">
         <figcaption >Credit: <i>https://www.webfx.com/blog/internet/what-is-a-web-crawler/</i></figcaption>
    </figure>

<main> <content id = "content">
        <p >In this article I will be looking at web robots; I will be discussing what they are, what they do and what they are used for. Web robots according to Ratan(2017) are software applications that travel across any web applications and perform a variety of automated tasks; I was at first confused when I first read this definition of what a web robot is, because I have always assumed that ‘robots’ were machines that one can physically see and maybe even touch. Web robots are again referred to as web spiders or web crawlers, and what they do is, they  take information from different web servers, analyse that information and then file it at a speed that is much faster than that of humans, if they were to perform the exact same task.  </p>
        
        <p >There are different types of web crawlers, there are good legitimate web robots and then there are bad web robots. The good crawlers serve as time savers for users, on different applications; examples of these good web robots include the spider web robots, the trading web robots and the media web robots. The  bad web crawlers, are used mostly by hackers; examples of such robots include spam web robots, hacker web robots, botnets, and download web robots. </p>
    
        <p >The advantages of web bots are that they help in performing tasks faster than humans, they help in retrieving data needed from large sets in the lest possible time. They are more reliable in the actions that they perform as compared to when the same task is done manually (Ratan V, 2017). The disadvantages, however, are that different malicious web bots are used by hackers to check for vulnerabilities in the web system, and they exploit the same for different malicious activities(Ratan V, 2017); these bots can be used to access users’ personal information, without the users’ permission. </p>
        
        <p>There are ways to avoid  having to deal with the problems associated with web robots, one way is by using meta tags to instruct the bots on how your site should be indexed or downloaded. Using different server-specific components is another way, this is where you write an extension to your web server which would dynamically verify different properties of calling programs to check for possible web bots(Ratan V, 2017). </p>
        
        </content>
</main>

<footer>
    <date> <i>Written on February 23,2020 </i> </date>
    
        <h2 style=> REFERENCES </h2>
        <ul>
            <li >
                <cite >https://opensourceforu.com/2017/07/web-robots-worker-bees-internet/</cite>
            </li>
        </ul>
    
        
        <div class = BacktoTop>

            <button onclick = "BacktoTop()" id ="topbtn" tittle = "Back to top">Back to Top</button>
    
        </div> 
</footer> 

<script>
            
    function BacktoTop() {
        document.body.scrollTop = 0
        document.documentElement.scrollTop = 0;
    }
        </script>

</body>

</html>